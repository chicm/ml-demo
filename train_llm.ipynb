{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Large Language Model step by step\n",
    "\n",
    "\n",
    "This notebook shows how to fine-tune a large language model (LLM) on a single system with 4 GPUs using Pytorch Fully Sharded Data Parallel (FSDP). For demonstration purposes, I use the Llama2-7B model and the IMDB sentiment analysis task, which reformulated as a generation task. \n",
    "\n",
    "By training Llama2-7B for one epoch (~25 minutes) in this notebook, we achieve <span style=\"color:#008bf8ff; font-weight: bold;\">96.86%</span> accuracy, surpassing the best model on this leaderboard: [IMDB benchmark](https://paperswithcode.com/sota/sentiment-analysis-on-imdb). You can reproduce the result by clicking `Run All`. \n",
    "\n",
    "This notebook requires pytorch installed and it installs all other dependencies by itself.\n",
    "\n",
    "****\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "- **Initializing the distributed environment**\n",
    "\n",
    "- **Loading and exploring a dataset**\n",
    "\n",
    "- **Designing a prompt to transform classification task into generation task**\n",
    "\n",
    "- **Loading training data in a distributed manner**\n",
    "\n",
    "- **Configuring the FSDP model wrapper**\n",
    "\n",
    "- **Configuring FSDP activation checkpointing**\n",
    "\n",
    "- **Saving sharded model weights to disk**\n",
    "\n",
    "- **Training LLM with FSDP**\n",
    "\n",
    "- **Evaluating generative LLM**\n",
    "\n",
    "****\n",
    "\n",
    "## Fully Sharded Data Parallel\n",
    "\n",
    "Fully Sharded Data Parallel (FSDP) shards data, model parameters, gradients and optimizer states to train very large model with limited resources, inspired by [Automatic Cross-Replica Sharding of Weight Update in Data-Parallel Training](https://arxiv.org/abs/2004.13336) and [DeepSpeed Zero](https://arxiv.org/abs/1910.02054). Reference [FSDP doc](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) to understand it. Here we give a brief introduction to FSDP.\n",
    "\n",
    "\n",
    "In standard [Distributed Data Parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) training, every worker processes a separate data batch with a whole model. The whole model weights, optimizer states and gradients replicated across all workers. For very large model, a single GPU may not have enough Memory to load all these tensors.\n",
    "\n",
    "FSDP shards data same as DDP, it also shards model parameters, optimizer states and gradients. Let's say there are 4 workers, each worker only hold 1/4 pieces of all tensors.\n",
    "\n",
    "FSDP also shards model vertically, it divides the whole model into FSDP units, during forward and backward passes, it executes unit by unit (typicall we put one layer into one unit for LLM). \n",
    "\n",
    "\n",
    "FSDP data and model split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/fsdp.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.34.0\n",
    "!pip install accelerate==0.22.0\n",
    "!pip install sentencepiece==0.1.99\n",
    "!pip install datasets==2.14.4\n",
    "!pip install seaborn==0.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize distributed environment\n",
    "\n",
    "We train LLM in a distributed way, so the first step is to initailize distributed environment. In below jupyter notebook cell, only one worker is initialied. At the end of this notebook, we lauch our training job with `torchrun` command to launch 4 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "world_size = dist.get_world_size()\n",
    "local_rank = dist.get_rank()\n",
    "\n",
    "print(world_size, local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IMDB dataset\n",
    "\n",
    "Firstly let's load the IMDB dataset and do minimum EDA.\n",
    "\n",
    "IMDB is a Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
    "\n",
    "Reference [IMDB](https://huggingface.co/datasets/imdb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"llama2/models_hf/7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = list(map(lambda x: len(x.split(' ')), dataset['train']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(num_words) / len(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_label_count(labels):\n",
    "    label_names =  ['negative', 'positive']\n",
    "    c = dict(Counter([label_names[x] for x in labels]))\n",
    "    data = {\n",
    "        'x': list(c.keys()),\n",
    "        'y': list(c.values())\n",
    "    }\n",
    "    sns.barplot(data, x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_label_count(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_label_count(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design a prompt to reformuate IMDB classification task to generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the prompts into 2 parts allows the truncation only being performed on the `text` part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_part1 = \\\n",
    "f'''Given a movie review/comment by a user in following format:\n",
    "#### Movie review:\n",
    "<review>\n",
    "#### Answer:\n",
    "<answer>\n",
    "Please rate the movie review as positive or negative from the perspective of the user's overall personal feelings to the movie. Answer it with only 'positive' or 'negative' without any explanation.\n",
    "\n",
    "#### Movie review:\n",
    "{{text}}\n",
    "'''\n",
    "\n",
    "prompt_part2 = \\\n",
    "f'''\n",
    "#### Answer: {{label}}'''\n",
    "\n",
    "prompt_part2_inference = \\\n",
    "'''\n",
    "#### Answer: '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_part1.format(text=dataset['train'][0]['text']) + prompt_part2.format(label='negative'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we use the above text as training data, by default we train the model on every tokens, the loss of every token are equally used to train the model, this is not what we want, we want to finetune the model only on the label token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual data for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_part1.format(text=dataset['train'][0]['text']) + prompt_part2_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "# we only finetune using the loss at label position, ignore other labels.\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "label_names = ['negative', 'positive']\n",
    "\n",
    "def preprocess_train(sample):\n",
    "    part1 = prompt_part1.format(text=sample['text'])\n",
    "    part2 = prompt_part2.format(label=label_names[sample['label']])\n",
    "    \n",
    "    tokenized = tokenizer('<s> ' + part1, part2, add_special_tokens=False, truncation='only_first', padding='max_length', max_length=MAX_LENGTH)\n",
    "    \n",
    "    labels = torch.tensor(copy.deepcopy(tokenized['input_ids']), dtype=torch.int64)\n",
    "    actual_token_len = sum(tokenized['attention_mask'])\n",
    "\n",
    "    labels[ :actual_token_len-1] = IGNORE_INDEX\n",
    "    labels[actual_token_len:] = IGNORE_INDEX\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(tokenized['input_ids'], dtype=torch.int64),\n",
    "        'attention_mask': torch.tensor(tokenized['attention_mask']),\n",
    "        'labels': labels\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset['train'].map(preprocess_train, remove_columns=['text', 'label']).with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure FSDP\n",
    "\n",
    "We do following confiuration for FSDP:\n",
    "\n",
    "* There are 32 decoder layers of Llama2 model, we wrap each layer into a FSDP unit, so that the model is sharded vertically.\n",
    "\n",
    "* Activation checkpointing: which reruns the forward pass for each unit during backward, this allows the intermediate tensors can be released from GPU while training in layer by layer manner.\n",
    "\n",
    "* Wrap the model into FSDP wrapper to enable FSDP for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    checkpoint_wrapper,\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing\n",
    ")\n",
    "\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import ShardingStrategy, FullStateDictConfig\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "fullstate_save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "\n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")\n",
    "\n",
    "check_fn = lambda submodule: isinstance(submodule, LlamaDecoderLayer)\n",
    "\n",
    "def get_llama_wrapper():\n",
    "    llama_auto_wrap_policy = partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            LlamaDecoderLayer,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return llama_auto_wrap_policy\n",
    "\n",
    "def apply_fsdp_checkpointing(model):\n",
    "    print(f\"--> applying fsdp activation checkpointing...\")\n",
    "\n",
    "    apply_activation_checkpointing(\n",
    "        model, checkpoint_wrapper_fn=non_reentrant_wrapper, check_fn=check_fn\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class fsdp_config:\n",
    "    mixed_precision: bool=True\n",
    "    use_fp16: bool=False\n",
    "    sharding_strategy: ShardingStrategy = ShardingStrategy.FULL_SHARD\n",
    "    checkpoint_type: StateDictType = StateDictType.SHARDED_STATE_DICT\n",
    "    fsdp_activation_checkpointing: bool=True\n",
    "    pure_bf16: bool = False\n",
    "    optimizer: str= \"AdamW\"\n",
    "\n",
    "def setup_fdsp_model():\n",
    "    model = LlamaForCausalLM.from_pretrained(MODEL_DIR, load_in_8bit=False, device_map=None, torch_dtype=torch.float16, use_cache=True)\n",
    "    model.to(torch.bfloat16)\n",
    "    model.config.pad_token_id = model.config.bos_token_id\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=get_llama_wrapper(),\n",
    "        mixed_precision=None,\n",
    "        sharding_strategy=fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "        sync_module_states=False,\n",
    "        param_init_fn=None\n",
    "    )\n",
    "    apply_fsdp_checkpointing(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model checkpoint during training\n",
    "\n",
    "FSDP model weights spreads across all ranks, so we need all rank run into this function to get a full state, then only rank 0 save it onto disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_model_checkpoint(\n",
    "    model, \n",
    "    output_dir,\n",
    "    rank\n",
    "):\n",
    "    \"\"\"saving model via rank0 cpu streaming and full_state_dict\"\"\"\n",
    "\n",
    "    with FSDP.state_dict_type(\n",
    "        model, StateDictType.FULL_STATE_DICT, fullstate_save_policy\n",
    "    ):\n",
    "        cpu_state = model.state_dict()\n",
    "\n",
    "        print(f\"saving process: rank {rank}  done w model state_dict\\n\")\n",
    "   \n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"--> saving model ...\")\n",
    "        save_dir = Path.cwd() / output_dir\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_full_path = str(save_dir) + \"/pytorch_model.bin\"\n",
    "\n",
    "        # save model\n",
    "        torch.save(cpu_state, save_full_path)\n",
    "        \n",
    "        print(f\"model checkpoint saved at {save_full_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "OUTPUT_DIR = './checkpoints_llama_1'\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "def train(model, train_dataloader, optimizer, local_rank):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for step, x in tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=(local_rank!=0), desc=f'Epoch {epoch}/{NUM_EPOCHS}'):\n",
    "            model.train()\n",
    "            x = BatchEncoding(x).to(local_rank)\n",
    "\n",
    "            loss = model(**x).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step % 50 == 0 and local_rank == 0:\n",
    "                print('train loss:', loss.item()) \n",
    "    save_model_checkpoint(model, OUTPUT_DIR, local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the training\n",
    "\n",
    "Now let's put all of above together and launch the training loop distributedly using torchrun command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n",
    "    checkpoint_wrapper,\n",
    "    CheckpointImpl,\n",
    "    apply_activation_checkpointing\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import ShardingStrategy, FullStateDictConfig\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n",
    "\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, default_data_collator\n",
    "\n",
    "fullstate_save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n",
    "\n",
    "non_reentrant_wrapper = partial(\n",
    "    checkpoint_wrapper,\n",
    "    checkpoint_impl=CheckpointImpl.NO_REENTRANT,\n",
    ")\n",
    "\n",
    "check_fn = lambda submodule: isinstance(submodule, LlamaDecoderLayer)\n",
    "\n",
    "\n",
    "MODEL_DIR = \"llama2/models_hf/7B\"\n",
    "NUM_EPOCHS = 1\n",
    "OUTPUT_DIR = './checkpoints_llama_1'\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "\n",
    "# initialize distributed environment\n",
    "dist.init_process_group(\"nccl\")\n",
    "world_size = dist.get_world_size()\n",
    "local_rank = dist.get_rank()\n",
    "torch.cuda.set_device(local_rank)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "\n",
    "prompt_part1 = \\\n",
    "f'''Given a movie review/comment by a user in following format:\n",
    "#### Movie review:\n",
    "<review>\n",
    "#### Answer:\n",
    "<answer>\n",
    "Please rate the movie review as positive or negative from the perspective of the user's overall personal feelings to the movie. Answer it with only 'positive' or 'negative' without any explanation.\n",
    "\n",
    "#### Movie review:\n",
    "{{text}}\n",
    "'''\n",
    "\n",
    "prompt_part2 = \\\n",
    "f'''\n",
    "#### Answer: {{label}}'''\n",
    "\n",
    "prompt_part2_inference = \\\n",
    "'''\n",
    "#### Answer: '''\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_DIR)\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "# we only finetune using the loss at label position, ignore other labels.\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "label_names = ['negative', 'positive']\n",
    "\n",
    "def preprocess_train(sample):\n",
    "    part1 = prompt_part1.format(text=sample['text'])\n",
    "    part2 = prompt_part2.format(label=label_names[sample['label']])\n",
    "    \n",
    "    tokenized = tokenizer('<s> ' + part1, part2, add_special_tokens=False, truncation='only_first', padding='max_length', max_length=MAX_LENGTH)\n",
    "    \n",
    "    labels = torch.tensor(copy.deepcopy(tokenized['input_ids']), dtype=torch.int64)\n",
    "    actual_token_len = sum(tokenized['attention_mask'])\n",
    "\n",
    "    labels[:actual_token_len-1] = IGNORE_INDEX\n",
    "    labels[actual_token_len:] = IGNORE_INDEX\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(tokenized['input_ids'], dtype=torch.int64),\n",
    "        'attention_mask': torch.tensor(tokenized['attention_mask']),\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def create_train_dataloader(train_ds, batch_size, local_rank):\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_ds,\n",
    "        rank=dist.get_rank(),\n",
    "        num_replicas=dist.get_world_size(),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    return train_dataloader\n",
    "\n",
    "\n",
    "def get_llama_wrapper():\n",
    "    llama_auto_wrap_policy = partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={\n",
    "            LlamaDecoderLayer,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return llama_auto_wrap_policy\n",
    "\n",
    "def apply_fsdp_checkpointing(model):\n",
    "    print(f\"--> applying fsdp activation checkpointing...\")\n",
    "\n",
    "    apply_activation_checkpointing(\n",
    "        model, checkpoint_wrapper_fn=non_reentrant_wrapper, check_fn=check_fn\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class fsdp_config:\n",
    "    mixed_precision: bool=True\n",
    "    use_fp16: bool=False\n",
    "    sharding_strategy: ShardingStrategy = ShardingStrategy.FULL_SHARD\n",
    "    checkpoint_type: StateDictType = StateDictType.SHARDED_STATE_DICT\n",
    "    fsdp_activation_checkpointing: bool=True\n",
    "    pure_bf16: bool = False\n",
    "    optimizer: str= \"AdamW\"\n",
    "\n",
    "def setup_fdsp_model():\n",
    "    model = LlamaForCausalLM.from_pretrained(MODEL_DIR, load_in_8bit=False, device_map=None, torch_dtype=torch.float16, use_cache=True)\n",
    "    model.to(torch.bfloat16)\n",
    "    model.config.pad_token_id = model.config.bos_token_id\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=get_llama_wrapper(),\n",
    "        mixed_precision=None,\n",
    "        sharding_strategy=fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "        sync_module_states=False,\n",
    "        param_init_fn=None\n",
    "    )\n",
    "    apply_fsdp_checkpointing(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model_checkpoint(\n",
    "    model, \n",
    "    output_dir,\n",
    "    rank\n",
    "):\n",
    "    \"\"\"saving model via rank0 cpu streaming and full_state_dict\"\"\"\n",
    "\n",
    "    with FSDP.state_dict_type(\n",
    "        model, StateDictType.FULL_STATE_DICT, fullstate_save_policy\n",
    "    ):\n",
    "        cpu_state = model.state_dict()\n",
    "\n",
    "        print(f\"saving process: rank {rank}  done w model state_dict\\n\")\n",
    "   \n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"--> saving model ...\")\n",
    "        save_dir = Path.cwd() / output_dir\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        save_full_path = str(save_dir) + \"/pytorch_model.bin\"\n",
    "\n",
    "        # save model\n",
    "        torch.save(cpu_state, save_full_path)\n",
    "        \n",
    "        print(f\"model checkpoint saved at {save_full_path}\\n\")\n",
    "\n",
    "def train(model, train_dataloader, optimizer, local_rank):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for step, x in tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=(local_rank!=0), desc=f'Epoch {epoch}/{NUM_EPOCHS}'):\n",
    "            model.train()\n",
    "            x = BatchEncoding(x).to(local_rank)\n",
    "\n",
    "            loss = model(**x).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step % 50 == 0 and local_rank == 0:\n",
    "                print('train loss:', loss.item()) \n",
    "    save_model_checkpoint(model, OUTPUT_DIR, local_rank)\n",
    "\n",
    "train_ds = dataset['train'].map(preprocess_train, remove_columns=['text', 'label']).with_format('torch')\n",
    "train_dataloader = create_train_dataloader(train_ds, BATCH_SIZE, local_rank)\n",
    "\n",
    "model = setup_fdsp_model()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.0)\n",
    "train(model, train_dataloader, optimizer, local_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!NCCL_DEBUG=WARN torchrun --nnodes 1 --nproc_per_node 4 train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "During training, we only saved `pytorch_model.bin` for simiplicity, we need to copy model configuration files from pretrain model directory to checkpoint directory.\n",
    "\n",
    "For this specific task, we only need to generate one token (`positive` or `negative`),  so calling model.forward is sufficient.\n",
    "\n",
    "For the task we need to generate multiple tokens,  we use `model.generate` to replace `model.forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $MODEL_DIR/*.json $OUTPUT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $OUTPUT_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(OUTPUT_DIR, load_in_8bit=False, device_map='cuda:0', torch_dtype=torch.float16, use_cache=True)\n",
    "model.to(torch.bfloat16)\n",
    "model.config.pad_token_id = model.config.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_eval(sample):\n",
    "    part1 = prompt_part1.format(text=sample['text'])\n",
    "    \n",
    "    return tokenizer('<s> ' + part1, prompt_part2_inference, add_special_tokens=False, truncation='only_first', padding='max_length', max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval = load_dataset('imdb', split='test')\n",
    "eval_ds = dataset_eval.map(preprocess_eval, remove_columns=['text', 'label']).with_format('torch')\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_ds,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode('negative positive', add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(eval_dataloader, total=len(eval_dataloader)):\n",
    "        model_inputs = BatchEncoding(x).to(0)\n",
    "        outputs = model(**model_inputs)\n",
    "        actual_token_len = model_inputs['attention_mask'].sum(-1).unsqueeze(1).unsqueeze(-1).expand(-1,-1,32000)\n",
    "        gathered = torch.gather(outputs.logits.detach(), dim=1, index=actual_token_len-1).squeeze(1).cpu()\n",
    "        result.append(torch.argmax(gathered.squeeze(1)[:, [8178, 6374]], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat(result, dim=0).numpy()\n",
    "labels = np.array(dataset_eval['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds == labels).sum() / len(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
