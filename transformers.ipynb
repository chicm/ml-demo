{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        #self.vocab_size = vocab_size\n",
    "        self.vocab = {'a': 0, 'b': 1, 'c': 2}\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return [self.vocab[i] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_heads=4):\n",
    "        super(AttentionLayer, self).__init__() \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_k = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_v = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape[:2]\n",
    "        head_size = self.hidden_size // self.num_heads\n",
    "        \n",
    "        q = self.fc_q(x).transpose(1,2).contiguous().view(bs, self.num_heads, seq_len,  head_size)\n",
    "        k = self.fc_k(x).transpose(1,2).contiguous().view(bs, self.num_heads, seq_len,  head_size)\n",
    "        v = self.fc_v(x).transpose(1,2).contiguous().view(bs, self.num_heads, seq_len,  head_size)\n",
    "        \n",
    "        attn = torch.softmax(torch.matmul(q, k.transpose(2,3) / 8), dim=-1)\n",
    "        \n",
    "        return torch.matmul(attn, v).transpose(1,2).contiguous().view(bs, seq_len, self.hidden_size)\n",
    "    \n",
    "    \n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_heads=4):\n",
    "        super(TransformerLayer, self).__init__() \n",
    "        self.attn_layer = AttentionLayer(hidden_size, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.attn_layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x + residual\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_heads=4, num_layers=10):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerLayer(hidden_size, num_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "\n",
    "x = torch.randn(2, 15, 64)\n",
    "\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 64])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = TransformerLayer()\n",
    "\n",
    "x = torch.randn(2, 15, 64)\n",
    "\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer = AttentionLayer()\n",
    "\n",
    "x = torch.randn(2, 15, 64)\n",
    "\n",
    "att_layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0780, -0.2307, -0.2466,  0.3914,  0.5816,  0.1741, -0.2141,\n",
       "           0.1885, -0.2946, -0.5029, -0.9384,  0.2282,  0.3352,  0.6129,\n",
       "           0.2073, -0.5081,  0.1921, -0.3360,  0.4406,  0.0495, -0.0905,\n",
       "          -0.1582,  0.7354, -0.5887, -0.5715,  0.0904,  0.4597,  0.3446,\n",
       "          -0.0194, -0.2957,  0.1726, -0.2321, -0.0954,  0.0038, -0.4618,\n",
       "          -0.1550, -0.5217,  0.4981,  0.4039,  0.2940,  0.4016,  0.5784,\n",
       "           0.8469,  0.2532, -0.6792, -0.7106,  0.1902,  0.1428, -0.5776,\n",
       "           0.5145,  0.2267,  0.4681,  0.0272,  0.7518, -0.7133,  0.0617,\n",
       "          -0.1920, -0.1159,  0.1193, -0.1058,  0.0744, -0.0966,  0.4280,\n",
       "           0.5555],\n",
       "         [ 0.0214, -0.2793, -0.2312,  0.3210,  0.5137,  0.0801, -0.0627,\n",
       "           0.1494, -0.4391, -0.5463, -0.9114,  0.1929,  0.4407,  0.6383,\n",
       "           0.2059, -0.4331,  0.4221, -0.0648,  0.1145,  0.0468, -0.1162,\n",
       "          -0.2959,  0.4275, -0.2808, -0.2020, -0.1531,  0.3467,  0.1109,\n",
       "           0.1768, -0.4886, -0.0441, -0.1882, -0.0800, -0.0854, -0.4874,\n",
       "          -0.2335, -0.3846,  0.3651,  0.3935,  0.1342,  0.2345,  0.4399,\n",
       "           0.9607,  0.0690, -0.3294, -0.7054,  0.1104, -0.3193, -0.2156,\n",
       "           0.2487,  0.1925,  0.7445,  0.6274,  0.1935, -0.4895, -0.0734,\n",
       "           0.0292,  0.1564,  0.1463, -0.2905,  0.0673, -0.0957,  0.3323,\n",
       "           0.2028],\n",
       "         [-0.1752, -0.3561, -0.2260,  0.3725,  0.7237, -0.0029, -0.0834,\n",
       "           0.0625, -0.3634, -0.6444, -0.8080,  0.1219,  0.4943,  0.6454,\n",
       "           0.4318, -0.1363,  0.3283, -0.1701,  0.0688,  0.1265, -0.0506,\n",
       "          -0.2159,  0.6010, -0.3864, -0.2598, -0.0580,  0.3703,  0.1471,\n",
       "           0.0297, -0.4015, -0.0337, -0.2169, -0.5243, -0.1377, -0.8870,\n",
       "          -0.4801,  0.3347, -0.0837,  0.4216, -0.1380,  0.1557,  0.5062,\n",
       "           0.4854,  0.3370, -0.4903, -0.2190, -0.0873, -0.4901, -0.0509,\n",
       "           0.4995,  0.0078,  0.3487,  0.3278,  0.4405, -0.5302, -0.1018,\n",
       "          -0.0769,  0.2635,  0.1228, -0.3636,  0.1434, -0.2877,  0.2506,\n",
       "           0.1743],\n",
       "         [ 0.4172, -0.2779, -0.2786,  0.5160,  0.6104,  0.1744, -0.3668,\n",
       "           0.3106, -0.1893, -0.6157, -1.3961,  0.3287,  0.5012,  0.4221,\n",
       "          -0.2400, -0.8463,  0.3029,  0.1314,  0.1897, -0.2602, -0.0947,\n",
       "          -0.2755,  0.1556, -0.0765, -0.1766, -0.3087,  0.5159,  0.0262,\n",
       "           0.4780, -0.4608, -0.0641, -0.0343, -0.2935,  0.1175, -0.5833,\n",
       "          -0.0736,  0.0402, -0.1814,  0.2766, -0.2463,  0.3422,  0.5811,\n",
       "           0.6303, -0.1657, -0.5685, -0.2274, -0.3900, -0.4318,  0.5704,\n",
       "           0.8994, -0.4155, -0.1921, -0.4488,  0.5437, -0.8853,  0.2434,\n",
       "           0.5044,  0.6034, -0.3190, -0.0411,  0.4512, -1.5169,  0.5892,\n",
       "           0.1183],\n",
       "         [ 0.3735, -0.1292, -0.1752,  0.5517,  0.5148,  0.1698, -0.3469,\n",
       "           0.2323, -0.2936, -0.5646, -1.1219,  0.3281,  0.3769,  0.5612,\n",
       "          -0.0370, -0.7102,  0.0813,  0.0967, -0.1858, -0.1531,  0.1195,\n",
       "          -0.1057,  0.2814, -0.0366, -0.0523, -0.3147,  0.6095, -0.1416,\n",
       "           0.3575, -0.3045, -0.2054,  0.0353, -0.3638, -0.0591, -0.6594,\n",
       "          -0.3885,  0.1393,  0.0967,  0.3551, -0.1560,  0.1674,  0.4662,\n",
       "           0.6227,  0.1943, -0.4441, -0.4856, -0.1481, -0.3766, -0.2598,\n",
       "           0.3009,  0.2119,  0.7222,  0.4537,  0.2718, -0.6218,  0.0608,\n",
       "           0.0915,  0.1263,  0.0724, -0.1749,  0.1059, -0.2254,  0.4364,\n",
       "           0.2965]],\n",
       "\n",
       "        [[ 0.0093, -0.1863,  0.1016,  0.2236,  0.7771,  0.5193,  0.2880,\n",
       "           0.8076, -0.2161, -0.8517, -0.1364, -0.5797, -0.4329,  0.2626,\n",
       "          -0.5925, -0.5194,  0.3176, -0.5751, -0.4151, -0.3906, -0.6134,\n",
       "          -0.3123, -0.9878,  0.4136, -0.3391, -0.0604,  0.3958, -0.2846,\n",
       "          -0.3869,  0.3321, -0.1632,  0.1066,  0.5951, -0.2831, -0.1253,\n",
       "          -0.3909,  0.3381, -0.0600,  0.3624, -0.1591, -0.2528,  0.1181,\n",
       "          -0.2829,  0.2282, -0.8970, -0.1299,  0.0101, -0.4149, -0.2483,\n",
       "          -0.5012, -0.1311, -0.1020,  0.1931, -0.5066,  0.4635, -0.4180,\n",
       "           0.1200, -0.0744,  0.8444, -0.4753, -0.1116,  0.0963,  0.1948,\n",
       "          -0.0466],\n",
       "         [-0.0575, -0.2032,  0.0889,  0.0429,  0.4463,  0.4345,  0.1988,\n",
       "           1.1478, -0.1417, -0.5993, -0.0378, -0.4997,  0.1410, -0.0093,\n",
       "          -0.6802, -0.1742,  0.0943, -0.2563, -0.2343, -0.5482, -0.7291,\n",
       "          -0.1007, -0.9533,  0.2277, -0.1277,  0.0674,  0.3069, -0.4960,\n",
       "          -0.4133,  0.2557,  0.0799,  0.1645,  0.6264, -0.6519, -0.1753,\n",
       "          -0.0664,  0.4920, -0.2612,  0.6422, -0.1827, -0.3474,  0.0476,\n",
       "          -0.4895,  0.3065, -0.6800,  0.2575,  0.0138, -0.3972, -0.1501,\n",
       "          -0.2414, -0.3311, -0.3874,  0.1831, -0.3143,  0.1290, -0.1829,\n",
       "           0.2132,  0.0342,  1.2885, -0.7522, -0.0839, -0.0192,  0.0398,\n",
       "          -0.2508],\n",
       "         [-0.1602,  0.2274, -0.3035,  0.5683,  0.2116,  0.3574,  0.4786,\n",
       "           0.8583,  0.1619, -0.3544, -0.5567, -0.9320, -0.2639,  0.0105,\n",
       "          -0.5052,  0.3636,  0.4643, -0.6243, -0.2968, -0.2252, -0.7007,\n",
       "          -0.0395, -1.1520,  0.3465, -0.4379, -0.1330,  0.3143, -0.4772,\n",
       "          -0.4724,  0.3662,  0.1073,  0.1889,  0.2662, -0.6768, -0.2737,\n",
       "           0.0535,  0.3289, -0.0050,  1.0219, -0.2240, -0.4249, -0.1094,\n",
       "          -0.6281,  0.0369, -0.6534,  0.5698,  0.1592, -0.5526, -0.2813,\n",
       "          -0.8325, -0.1604, -0.1776,  0.4200, -0.3862,  0.3482, -0.5085,\n",
       "          -0.0204, -0.0684,  0.8871, -0.2103, -0.2449,  0.4727,  0.2940,\n",
       "          -0.2124],\n",
       "         [-0.1562,  0.2348, -0.1518,  0.6448,  0.2016,  0.3007,  0.5278,\n",
       "           0.8813,  0.0930, -0.3128, -0.5440, -1.0942, -0.2852,  0.0104,\n",
       "          -0.4799,  0.4480,  0.6235, -0.6328, -0.0346, -0.1995, -0.6864,\n",
       "          -0.0218, -0.9411,  0.3430, -0.2603,  0.1786,  0.4563, -0.3915,\n",
       "          -0.4103,  0.2060, -0.2650,  0.2804,  0.3110, -0.2005, -0.3017,\n",
       "          -0.2745,  0.3696,  0.3144,  0.3908,  0.1219,  0.0361,  0.1539,\n",
       "          -0.1569,  0.2477, -0.7267, -0.0397,  0.1255, -0.5123, -0.4995,\n",
       "          -0.4013, -0.1866, -0.2238, -0.3506, -0.3589,  0.5003, -0.1777,\n",
       "           0.4271,  0.0678,  0.8406, -0.3097,  0.4309, -0.3005,  0.0635,\n",
       "          -0.5443],\n",
       "         [-0.0775,  0.0191, -0.1013,  0.3832,  0.4663,  0.4292,  0.3777,\n",
       "           0.8594, -0.0240, -0.5831, -0.3399, -0.7514, -0.3041,  0.1149,\n",
       "          -0.5576, -0.0506,  0.5055, -0.6246, -0.1507, -0.2169, -0.5944,\n",
       "           0.1482, -1.0614,  0.2255, -0.4679, -0.0317,  0.3341, -0.5036,\n",
       "          -0.4883,  0.3127,  0.0586,  0.2145,  0.1691, -0.4712, -0.3445,\n",
       "           0.0418,  0.4418,  0.0241,  0.6511,  0.1533, -0.2384,  0.0582,\n",
       "          -0.2243,  0.3272, -0.5154,  0.4922,  0.0183, -0.5320, -0.3651,\n",
       "          -0.6624, -0.2019, -0.4131,  0.0893, -0.4473,  0.2195, -0.2923,\n",
       "           0.2086, -0.0625,  0.8863, -0.7621, -0.1279, -0.1077,  0.1519,\n",
       "          -0.2170]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(2, 5, 4, 16).transpose(2, 1)\n",
    "k = torch.randn(2, 5, 4, 16).transpose(2, 1)\n",
    "v = torch.randn(2, 5, 4, 16).transpose(2, 1)\n",
    "\n",
    "#k.transpose(2, 1).shape\n",
    "attn = torch.softmax(torch.matmul(q, k.transpose(2,3) / 8), dim=-1)\n",
    "\n",
    "\n",
    "#torch.matmul(attn, v).shape\n",
    "#attn.shape\n",
    "torch.matmul(attn, v).transpose(1,2).contiguous().view(2, 5, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Modoule):\n",
    "    def __init__(self, num_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
